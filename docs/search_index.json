[["index.html", "ARTNeT Advanced Workshop on Analysing Trade and Trade Policy with the Structural Gravity Model Chapter 1 Introduction 1.1 Usual disclaimer 1.2 Background 1.3 Getting the most out of this material", " ARTNeT Advanced Workshop on Analysing Trade and Trade Policy with the Structural Gravity Model Mauricio “Pachá” Vargas Sepúlveda 2021-12-29 Chapter 1 Introduction 1.1 Usual disclaimer The views and opinions expressed in this course are solely those of the author and do not necessarily reflect the official position of any unit of the United Nations, the University of Toronto or the Pontifical Catholic University of Chile. 1.2 Background This unit of the workshop is based on Yotov et al. (2016). 1.3 Getting the most out of this material You can clone the GitHub repository to obtain the editable R files: git clone https://github.com/pachamaltese/unescap-gravity-2020.git Please read Bryan, Hester, and STAT 545 TAs (2020) if you have questions about git or GitHub. References "],["packages-and-data.html", "Chapter 2 Packages and data 2.1 Packages 2.2 Data", " Chapter 2 Packages and data 2.1 Packages Required packages for this workshop: library(haven) # read dta format (Stata) library(janitor) # tidy column names library(dplyr) # chained operations library(sandwich) # covariance based estimators library(lmtest) # econometric tests library(broom) # tidy regression results 2.2 Data We can read directly from Stata files: gravity &lt;- clean_names(read_dta(&quot;data/gravity-data.dta&quot;)) Now we need to prepare interval data: gravity2 &lt;- gravity %&gt;% filter(year %in% seq(1986, 2006, 4)) We are going to need to create and transform some variables that are needed later: gravity2 &lt;- gravity2 %&gt;% mutate( log_trade = log(trade), log_dist = log(dist) ) %&gt;% group_by(exporter, year) %&gt;% mutate( output = sum(trade), log_output = log(output) ) %&gt;% group_by(importer, year) %&gt;% mutate( expenditure = sum(trade), log_expenditure = log(expenditure) ) %&gt;% ungroup() Before concluiding data preparation, we need to create pair ID and symmetric pair ID variables. IMPORTANT: Here we don’t need to create pair_id and symm_id as in Stata, the process is much simpler here (but other tasks will be harder!) gravity2 &lt;- gravity2 %&gt;% mutate( pair = paste(exporter, importer, sep = &quot;_&quot;), first = ifelse(exporter &lt; importer, exporter, importer), second = ifelse(exporter &lt; importer, importer, exporter), symm = paste(first, second, sep = &quot;_&quot;) ) "],["ols-estimates.html", "Chapter 3 OLS estimates 3.1 Adjust 3.2 Compute clustered standard errors 3.3 RESET test", " Chapter 3 OLS estimates The general equation for this model is: \\[ \\begin{align} \\log X_{ij,t} =&amp; \\:\\beta_0 + \\beta_1 DIST_{i,j} + \\beta_2 CNTG_{i,j} + \\beta_3 LANG_{i,j} + \\beta_4 CLNY_{i,j} + \\beta_5 \\log Y_{i,t} +\\\\ \\text{ }&amp; \\beta_6 \\log E_{j,t} + \\varepsilon_{ij,t} \\end{align} \\] We start by removing 0 flows. IMPORTANT: If we don’t do this, lm fails because log(0) = -Inf. gravity2 &lt;- gravity2 %&gt;% filter(exporter != importer, trade &gt; 0) 3.1 Adjust We start with a linear model with usual standard errors: model1 &lt;- lm(log_trade ~ log_dist + cntg + lang + clny + log_output + log_expenditure, data = gravity2) summary(model1) # no clustered std error here! ## ## Call: ## lm(formula = log_trade ~ log_dist + cntg + lang + clny + log_output + ## log_expenditure, data = gravity2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.5421 -0.8281 0.1578 1.0476 7.6585 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -11.283080 0.151732 -74.36 &lt; 2e-16 *** ## log_dist -1.001607 0.014159 -70.74 &lt; 2e-16 *** ## cntg 0.573805 0.074427 7.71 1.31e-14 *** ## lang 0.801548 0.033748 23.75 &lt; 2e-16 *** ## clny 0.734853 0.070387 10.44 &lt; 2e-16 *** ## log_output 1.190236 0.005402 220.32 &lt; 2e-16 *** ## log_expenditure 0.907588 0.005577 162.73 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.743 on 25682 degrees of freedom ## Multiple R-squared: 0.7585, Adjusted R-squared: 0.7585 ## F-statistic: 1.345e+04 on 6 and 25682 DF, p-value: &lt; 2.2e-16 3.2 Compute clustered standard errors In R we can work on top of the previous model to obtain a clustered variance matrix: vcov_cluster1 &lt;- vcovCL(model1, cluster = gravity2[, &quot;pair&quot;], df_correction = TRUE) coef_test1 &lt;- tidy(coeftest(model1, vcov_cluster1)) From here we can obtain correct (clustered) F distribution based statistics wald1 &lt;- tidy(waldtest(model1, vcov = vcov_cluster1, test = &quot;F&quot;)) fs1 &lt;- wald1$statistic[2] fp1 &lt;- wald1$p.value[2] Same for obtaining the root MSE rss1 &lt;- as.numeric(crossprod(model1$residuals)) rmse1 &lt;- sqrt(rss1 / length(model1$residuals)) Now we can create a list with all the previous information: model1_results &lt;- list( summary = tibble( n_obs = nrow(gravity2), f_stat = fs1, prob_f = fp1, r_sq = summary(model1)$r.squared, root_mse = rmse1 ), coefficients = coef_test1 ) model1_results ## $summary ## # A tibble: 1 × 5 ## n_obs f_stat prob_f r_sq root_mse ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 25689 4893. 0 0.759 1.74 ## ## $coefficients ## # A tibble: 7 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -11.3 0.256 -44.1 0 ## 2 log_dist -1.00 0.0230 -43.5 0 ## 3 cntg 0.574 0.137 4.18 2.90e- 5 ## 4 lang 0.802 0.0682 11.8 7.88e-32 ## 5 clny 0.735 0.114 6.46 1.05e-10 ## 6 log_output 1.19 0.00931 128. 0 ## 7 log_expenditure 0.908 0.00973 93.3 0 3.3 RESET test It is (extremely) important to conduct a misspecification test, this is easier to do in Stata but we can still work it out in R: gravity2 &lt;- gravity2 %&gt;% mutate(fit2 = predict(model1)^2) model2 &lt;- lm(log_trade ~ log_dist + cntg + lang + clny + log_output + log_expenditure + fit2, data = gravity2) vcov_cluster2 &lt;- vcovCL(model2, cluster = gravity2[, &quot;pair&quot;], df_correction = TRUE) coef_test2 &lt;- tidy(coeftest(model2, vcov_cluster2)) coef_test2 ## # A tibble: 8 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -11.6 0.275 -42.3 0 ## 2 log_dist -1.05 0.0252 -41.6 0 ## 3 cntg 0.702 0.133 5.28 1.28e- 7 ## 4 lang 0.835 0.0682 12.2 2.51e-34 ## 5 clny 0.797 0.110 7.26 4.04e-13 ## 6 log_output 1.24 0.0143 86.4 0 ## 7 log_expenditure 0.944 0.0135 70.1 0 ## 8 fit2 -0.00699 0.00143 -4.88 1.04e- 6 "],["remoteness-estimates.html", "Chapter 4 Remoteness estimates 4.1 Create remoteness indexes 4.2 Adjust 4.3 Compute clustered standard errors 4.4 RESET test", " Chapter 4 Remoteness estimates The remoteness model adds variables to the OLS model. The general equation for this model is: \\[ \\begin{align} \\log X_{ij,t} =&amp; \\:\\beta_0 + \\beta_1 DIST_{i,j} + \\beta_2 CNTG_{i,j} + \\beta_3 LANG_{i,j} + \\beta_4 CLNY_{i,j} + \\beta_5 \\log Y_{i,t} +\\\\ \\text{ }&amp; \\beta_6 \\log E_{j,t} + \\beta_7 \\log(REM\\_EXP_i,t) + \\beta_8 \\log(REM\\_IMP_i,t) + \\varepsilon_{ij,t} \\end{align} \\] Where \\[ \\log(REM\\_EXP_{i,t}) = \\log \\left( \\sum_j \\frac{DIST_{i,j}}{E_{j,t} / Y_t} \\right)\\\\ \\log(REM\\_IMP_{j,t}) = \\log \\left( \\sum_i \\frac{DIST_{i,j}}{Y_{i,t} / Y_t} \\right) \\] 4.1 Create remoteness indexes We can create each index in a new table and then join the results: gravity2_rem_exp &lt;- gravity2 %&gt;% select(exporter, year, expenditure, dist) %&gt;% group_by(exporter, year) %&gt;% summarise(rem_exp = log(weighted.mean(dist, expenditure))) gravity2_rem_imp &lt;- gravity2 %&gt;% select(importer, year, output, dist) %&gt;% group_by(importer, year) %&gt;% summarise(rem_imp = log(weighted.mean(dist, output))) gravity2 &lt;- gravity2 %&gt;% left_join(gravity2_rem_exp) %&gt;% left_join(gravity2_rem_imp) 4.2 Adjust rem_formula &lt;- as.formula(&quot;log_trade ~ log_dist + cntg + lang + clny + log_output + log_expenditure + rem_exp + rem_imp&quot;) model3 &lt;- lm(rem_formula, data = gravity2) 4.3 Compute clustered standard errors vcov_cluster3 &lt;- vcovCL(model3, cluster = gravity2[, &quot;pair&quot;], df_correction = TRUE) coef_test3 &lt;- tidy(coeftest(model3, vcov_cluster3)) wald3 &lt;- tidy(waldtest(model3, vcov = vcov_cluster3, test = &quot;F&quot;)) fs3 &lt;- wald3$statistic[2] fp3 &lt;- wald3$p.value[2] rss3 &lt;- as.numeric(crossprod(model3$residuals)) rmse3 &lt;- sqrt(rss3 / length(model3$residuals)) model3_results &lt;- list( summary = tibble( n_obs = nrow(gravity2), f_stat = fs3, prob_f = fp3, r_sq = summary(model3)$r.squared, root_mse = rmse3 ), coefficients = coef_test3 ) model3_results ## $summary ## # A tibble: 1 × 5 ## n_obs f_stat prob_f r_sq root_mse ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 25689 3821. 0 0.761 1.73 ## ## $coefficients ## # A tibble: 9 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -21.8 1.18 -18.5 1.39e- 75 ## 2 log_dist -1.18 0.0304 -38.7 3.52e-319 ## 3 cntg 0.283 0.132 2.15 3.18e- 2 ## 4 lang 0.649 0.0689 9.43 4.40e- 21 ## 5 clny 0.876 0.117 7.48 7.87e- 14 ## 6 log_output 1.20 0.00936 128. 0 ## 7 log_expenditure 0.923 0.00985 93.7 0 ## 8 rem_exp 0.624 0.0869 7.18 7.09e- 13 ## 9 rem_imp 0.693 0.0977 7.09 1.34e- 12 4.4 RESET test gravity2 &lt;- gravity2 %&gt;% mutate(fit2 = predict(model3)^2) model4 &lt;- lm(update(rem_formula, ~ . + fit2), data = gravity2) vcov_cluster4 &lt;- vcovCL(model4, cluster = gravity2[, &quot;pair&quot;], df_correction = TRUE) coef_test4 &lt;- tidy(coeftest(model4, vcov_cluster4)) coef_test4 ## # A tibble: 10 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -22.7 1.19 -19.0 2.85e- 80 ## 2 log_dist -1.24 0.0326 -37.9 2.74e-306 ## 3 cntg 0.390 0.128 3.05 2.27e- 3 ## 4 lang 0.674 0.0687 9.82 9.95e- 23 ## 5 clny 0.947 0.113 8.42 4.01e- 17 ## 6 log_output 1.25 0.0143 87.2 0 ## 7 log_expenditure 0.960 0.0135 71.2 0 ## 8 rem_exp 0.662 0.0866 7.65 2.10e- 14 ## 9 rem_imp 0.722 0.0974 7.40 1.36e- 13 ## 10 fit2 -0.00702 0.00139 -5.04 4.64e- 7 "],["fixed-effects-estimates.html", "Chapter 5 Fixed effects estimates 5.1 Create fixed effects 5.2 Adjust 5.3 Check for collinear terms 5.4 Compute clustered standard errors", " Chapter 5 Fixed effects estimates 5.1 Create fixed effects This step is very different compared to Stata. By using Stata you need to create 830 0-1 columns, here you only need 2 factor columns: gravity2 &lt;- gravity2 %&gt;% mutate( exp_time = as.factor(paste(exporter, year, sep = &quot;_&quot;)), imp_time = as.factor(paste(importer, year, sep = &quot;_&quot;)) ) Now we see how many dummy variables we are adding to the OLS model, there’s a high risk of collinearity!: length(levels(gravity2$exp_time)) + length(levels(gravity2$imp_time)) ## [1] 828 5.2 Adjust fe_formula &lt;- as.formula(&quot;log_trade ~ log_dist + cntg + lang + clny + exp_time + imp_time&quot;) model5 &lt;- lm(fe_formula, data = gravity2) 5.3 Check for collinear terms collinear_terms &lt;- alias(model5) collinear_matrix &lt;- collinear_terms$Complete rownames(collinear_matrix) ## [1] &quot;imp_timeZAF_1990&quot; &quot;imp_timeZAF_1994&quot; &quot;imp_timeZAF_1998&quot; &quot;imp_timeZAF_2002&quot; &quot;imp_timeZAF_2006&quot; 5.4 Compute clustered standard errors vcov_cluster5 &lt;- vcovCL(model5, cluster = gravity2[, &quot;pair&quot;], df_correction = TRUE) coef_test5 &lt;- tidy(coeftest( model5, vcov_cluster5[ which(!grepl(&quot;^exp_time|^imp_time&quot;, rownames(vcov_cluster5))), which(!grepl(&quot;^exp_time|^imp_time&quot;, colnames(vcov_cluster5))) ] )) coef_test5 ## # A tibble: 5 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 13.1 0.444 29.4 2.68e-187 ## 2 log_dist -1.22 0.0308 -39.5 0 ## 3 cntg 0.223 0.151 1.48 1.38e- 1 ## 4 lang 0.661 0.0676 9.78 1.49e- 22 ## 5 clny 0.670 0.116 5.78 7.51e- 9 summary(model5)$r.squared ## [1] 0.8432398 "],["references.html", "Chapter 6 References", " Chapter 6 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
